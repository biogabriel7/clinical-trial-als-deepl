{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "/Users/gabrielduarte/Documents/GitHub/clinical-trial-als-deepl\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run this first)\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "# !pip install pycox torchtuples tqdm\n",
    "\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device for MacBook Pro M3\n",
    "device = 'mps' if tc.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tc.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "os.chdir('/Users/gabrielduarte/Documents/GitHub/clinical-trial-als-deepl')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  core_file: db/core_trial_info.csv\n",
      "  target_column: reached_phase_3_plus\n",
      "  id_column: Trial.ID\n",
      "  hidden_depth_simple: 0\n",
      "  factor_hidden_nodes: 4\n",
      "  device: mps\n",
      "  batch_size: 32\n",
      "  reduce_lr_epochs: [30, 30, 30]\n",
      "  lr: 0.001\n",
      "  dropout: 0.3\n",
      "  input_dropout: 0.3\n",
      "  weight_decay: 0.0001\n",
      "  splits: 3\n",
      "  lrp_gamma: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Configuration for ALS Clinical Trials Analysis\n",
    "config = {\n",
    "    # Data params\n",
    "    'core_file': 'db/core_trial_info.csv',\n",
    "    'target_column': 'reached_phase_3_plus',\n",
    "    'id_column': 'Trial.ID',\n",
    "    \n",
    "    # Model params (optimized for smaller dataset and M3 Mac)\n",
    "    'hidden_depth_simple': 0,   \n",
    "    'factor_hidden_nodes': 4,  # Reduced for smaller dataset\n",
    "    'device': device,\n",
    "    'batch_size': 32,  # Smaller batch size for M3\n",
    "    'reduce_lr_epochs': [30, 30, 30],  # Fewer epochs\n",
    "    'lr': 1e-3,\n",
    "    'dropout': 0.3,\n",
    "    'input_dropout': 0.3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'splits': 3,  # 3-fold CV for smaller dataset\n",
    "    'lrp_gamma': 0.01\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ALS Clinical Trials data...\n",
      "\n",
      "Dataset Overview:\n",
      "- Shape: (962, 14)\n",
      "- Missing values: 256 (1.9%)\n",
      "\n",
      "Target Variable Distribution:\n",
      "reached_phase_3_plus\n",
      "0    743\n",
      "1    219\n",
      "Name: count, dtype: int64\n",
      "Success rate: 22.8%\n",
      "\n",
      "Trial Phase Distribution:\n",
      "Trial Phase\n",
      "Ii        337\n",
      "I         296\n",
      "Iii       177\n",
      "Iv         82\n",
      "Ii/Iii     42\n",
      "Other      16\n",
      "(N/A)      12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values by column:\n",
      "Study Keywords     171\n",
      "Trial Objective      1\n",
      "Study Design        84\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_and_analyze_data(filepath):\n",
    "    \"\"\"Load and analyze the core trial data\"\"\"\n",
    "    print(\"Loading ALS Clinical Trials data...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"- Shape: {df.shape}\")\n",
    "    print(f\"- Missing values: {df.isnull().sum().sum()} ({df.isnull().sum().sum()/df.size*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTarget Variable Distribution:\")\n",
    "    target_dist = df[config['target_column']].value_counts()\n",
    "    print(target_dist)\n",
    "    print(f\"Success rate: {target_dist[1]/len(df)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nTrial Phase Distribution:\")\n",
    "    print(df['Trial Phase'].value_counts())\n",
    "    \n",
    "    print(f\"\\nMissing values by column:\")\n",
    "    missing = df.isnull().sum()\n",
    "    print(missing[missing > 0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = load_and_analyze_data(config['core_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6 feature columns\n",
      "After encoding: 379 features\n",
      "\n",
      "Final dataset shape: X=(962, 379), y=(962,)\n",
      "Feature names: ['Disease_Autoimmune/Inflammation: Asthma; CNS: Amyotrophic Lateral Sclerosis; CNS: Attention Deficit Hyperactive Disorder', 'Disease_Autoimmune/Inflammation: Psoriasis; Autoimmune/Inflammation: Rheumatoid Arthritis; Autoimmune/Inflammation: Ulcerative Colitis; Cardiovascular: Peripheral Arterial Disease; CNS: Amyotrophic Lateral Sclerosis', 'Disease_CNS: Alcohol Dependence; CNS: Amyotrophic Lateral Sclerosis; CNS: Anxiety; CNS: Multiple Sclerosis', 'Disease_CNS: Alcohol Dependence; CNS: Amyotrophic Lateral Sclerosis; CNS: Multiple Sclerosis; Oncology: CNS, Glioblastoma; Oncology: Colorectal; Oncology: Supportive Care', \"Disease_CNS: Alzheimer's Disease\"]...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_als_data(df, config):\n",
    "    \"\"\"Preprocess ALS clinical trials data for neural network\"\"\"\n",
    "    \n",
    "    # Columns to exclude from features\n",
    "    exclude_cols = [\n",
    "        config['target_column'], \n",
    "        config['id_column'], \n",
    "        'Protocol/Trial ID', \n",
    "        'Record URL', \n",
    "        'Trial Title',\n",
    "        'Trial Objective',  # Too many unique values\n",
    "        'Study Design',     # Too many unique values\n",
    "        'MeSH Term'        # Too many unique values\n",
    "    ]\n",
    "    \n",
    "    # Select feature columns\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    print(f\"Using {len(feature_cols)} feature columns\")\n",
    "    \n",
    "    # Prepare feature data\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[config['target_column']].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in X.select_dtypes(include=['object']).columns:\n",
    "        X[col] = X[col].fillna('Missing')\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "    print(f\"After encoding: {X_encoded.shape[1]} features\")\n",
    "    \n",
    "    # Convert to numpy and normalize\n",
    "    X_array = X_encoded.values.astype(np.float32)\n",
    "    y_array = y.values.astype(np.float32)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_array)\n",
    "    \n",
    "    # Create phase groups for stratification\n",
    "    phase_groups = df['Trial Phase'].values\n",
    "    \n",
    "    return X_scaled, y_array, phase_groups, X_encoded.columns.tolist(), scaler\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, phase_groups, feature_names, scaler = preprocess_als_data(df, config)\n",
    "print(f\"\\nFinal dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Feature names: {feature_names[:5]}...\")  # Show first 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network classes defined âœ“\n"
     ]
    }
   ],
   "source": [
    "class LRP_Linear(nn.Module):\n",
    "    def __init__(self, inp, outp, gamma=0.01, eps=1e-5):\n",
    "        super(LRP_Linear, self).__init__()\n",
    "        self.A_dict = {}\n",
    "        self.linear = nn.Linear(inp, outp)\n",
    "        nn.init.xavier_uniform_(self.linear.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        self.gamma = tc.tensor(gamma)\n",
    "        self.eps = tc.tensor(eps)\n",
    "        self.iteration = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            self.A_dict[self.iteration] = x.clone()\n",
    "        return self.linear(x)\n",
    "\n",
    "    def relprop(self, R):\n",
    "        device = next(self.parameters()).device\n",
    "        A = self.A_dict[self.iteration].clone()\n",
    "        A, self.eps = A.to(device), self.eps.to(device)\n",
    "\n",
    "        Ap = A.clamp(min=0).detach().data.requires_grad_(True)\n",
    "        Am = A.clamp(max=0).detach().data.requires_grad_(True)\n",
    "\n",
    "        zpp = self.newlayer(1).forward(Ap)  \n",
    "        zmm = self.newlayer(-1, no_bias=True).forward(Am) \n",
    "        zmp = self.newlayer(1, no_bias=True).forward(Am) \n",
    "        zpm = self.newlayer(-1).forward(Ap) \n",
    "\n",
    "        with tc.no_grad():\n",
    "            Y = self.forward(A).data\n",
    "\n",
    "        sp = ((Y > 0).float() * R / (zpp + zmm + self.eps * ((zpp + zmm == 0).float() + tc.sign(zpp + zmm)))).data\n",
    "        sm = ((Y < 0).float() * R / (zmp + zpm + self.eps * ((zmp + zpm == 0).float() + tc.sign(zmp + zpm)))).data\n",
    "\n",
    "        (zpp * sp).sum().backward()\n",
    "        cpp = Ap.grad\n",
    "        Ap.grad = None\n",
    "        Ap.requires_grad_(True)\n",
    "\n",
    "        (zpm * sm).sum().backward()\n",
    "        cpm = Ap.grad\n",
    "        Ap.grad = None\n",
    "        Ap.requires_grad_(True)\n",
    "\n",
    "        (zmp * sm).sum().backward()\n",
    "        cmp = Am.grad\n",
    "        Am.grad = None\n",
    "        Am.requires_grad_(True)\n",
    "\n",
    "        (zmm * sp).sum().backward()\n",
    "        cmm = Am.grad\n",
    "        Am.grad = None\n",
    "        Am.requires_grad_(True)\n",
    "\n",
    "        R_1 = (Ap * cpp).data\n",
    "        R_2 = (Ap * cpm).data\n",
    "        R_3 = (Am * cmp).data\n",
    "        R_4 = (Am * cmm).data\n",
    "\n",
    "        return R_1 + R_2 + R_3 + R_4\n",
    "\n",
    "    def newlayer(self, sign, no_bias=False):\n",
    "        if sign == 1:\n",
    "            rho = lambda p: p + self.gamma * p.clamp(min=0)\n",
    "        else:\n",
    "            rho = lambda p: p + self.gamma * p.clamp(max=0)\n",
    "\n",
    "        layer_new = copy.deepcopy(self.linear)\n",
    "        try:\n",
    "            layer_new.weight = nn.Parameter(rho(self.linear.weight))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            layer_new.bias = nn.Parameter(self.linear.bias * 0 if no_bias else rho(self.linear.bias))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return layer_new\n",
    "\n",
    "class LRP_ReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LRP_ReLU, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x)\n",
    "\n",
    "    def relprop(self, R):\n",
    "        return R\n",
    "\n",
    "class LRP_DropOut(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(LRP_DropOut, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)\n",
    "\n",
    "    def relprop(self, R):\n",
    "        return R\n",
    "\n",
    "class ALS_Model(nn.Module):\n",
    "    def __init__(self, n_features, config):\n",
    "        super(ALS_Model, self).__init__()\n",
    "        self.classname = 'ALS Clinical Trials Model'\n",
    "        \n",
    "        inp = n_features\n",
    "        hidden = int(config['factor_hidden_nodes'] * n_features)\n",
    "        outp = 1\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            LRP_DropOut(p=config['input_dropout']), \n",
    "            LRP_Linear(inp, hidden, gamma=config['lrp_gamma']), \n",
    "            LRP_ReLU()\n",
    "        )\n",
    "        \n",
    "        # Add hidden layers if specified\n",
    "        for i in range(config['hidden_depth_simple']):\n",
    "            self.layers.add_module(f'dropout_{i}', LRP_DropOut(p=config['dropout']))\n",
    "            self.layers.add_module(f'linear_{i}', LRP_Linear(hidden, hidden, gamma=config['lrp_gamma']))\n",
    "            self.layers.add_module(f'relu_{i}', LRP_ReLU())\n",
    "        \n",
    "        self.layers.add_module('dropout_final', LRP_DropOut(p=config['dropout']))\n",
    "        self.layers.add_module('linear_final', LRP_Linear(hidden, outp, gamma=config['lrp_gamma']))\n",
    "        \n",
    "        # Count parameters\n",
    "        nparams = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f'{self.classname} contains {nparams:,} trainable parameters')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers.forward(x)\n",
    "\n",
    "    def relprop(self, R):\n",
    "        assert not self.training, 'relprop does not work during training time'\n",
    "        for module in self.layers[::-1]:\n",
    "            R = module.relprop(R)\n",
    "        return R\n",
    "\n",
    "print(\"Neural network classes defined âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined âœ“\n"
     ]
    }
   ],
   "source": [
    "import torchtuples as tt\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, model, config):\n",
    "    \"\"\"Train the ALS model using Cox proportional hazards\"\"\"\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = tc.tensor(X_train).float().to(config['device'])\n",
    "    X_val_tensor = tc.tensor(X_val).float().to(config['device'])\n",
    "    \n",
    "    # Create survival data (using target as both duration and event)\n",
    "    train_durations = tc.tensor(y_train).float()\n",
    "    train_events = tc.tensor(y_train).bool()\n",
    "    val_durations = tc.tensor(y_val).float()\n",
    "    val_events = tc.tensor(y_val).bool()\n",
    "    \n",
    "    # Prepare data for pycox\n",
    "    train_data = (X_train_tensor, (train_durations, train_events))\n",
    "    val_data = (X_val_tensor, (val_durations, val_events))\n",
    "    \n",
    "    # Create CoxPH model\n",
    "    optimizer = tt.optim.Adam(config['lr'], weight_decay=config['weight_decay'])\n",
    "    coxph_model = CoxPH(model, optimizer, device=config['device'])\n",
    "    \n",
    "    # Training with learning rate reduction\n",
    "    all_logs = []\n",
    "    for exp, training_epochs in enumerate(config['reduce_lr_epochs']):\n",
    "        print(f'Training for {training_epochs} epochs with lr {config[\"lr\"]*10**(-exp):.6f}')\n",
    "        coxph_model.optimizer.set_lr(config['lr']*10**(-exp))\n",
    "        \n",
    "        callbacks = [\n",
    "            tt.callbacks.EarlyStopping(patience=10),\n",
    "            tt.callbacks.ClipGradNorm(model, max_norm=1.0)\n",
    "        ]\n",
    "        \n",
    "        log = coxph_model.fit(\n",
    "            X_train_tensor, (train_durations, train_events),\n",
    "            batch_size=config['batch_size'], \n",
    "            epochs=training_epochs, \n",
    "            callbacks=callbacks,\n",
    "            val_data=val_data, \n",
    "            val_batch_size=config['batch_size'], \n",
    "            verbose=True\n",
    "        )\n",
    "        all_logs.append(log)\n",
    "    \n",
    "    return coxph_model, all_logs\n",
    "\n",
    "def evaluate_model(coxph_model, X_test, y_test):\n",
    "    \"\"\"Evaluate the trained model\"\"\"\n",
    "    X_test_tensor = tc.tensor(X_test).float().to(config['device'])\n",
    "    test_durations = tc.tensor(y_test).float()\n",
    "    test_events = tc.tensor(y_test).bool()\n",
    "    \n",
    "    # Compute baseline hazards and predictions\n",
    "    _ = coxph_model.compute_baseline_hazards()\n",
    "    surv_pred = coxph_model.predict_surv_df(X_test_tensor)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    ev = EvalSurv(surv_pred, test_durations.numpy(), test_events.numpy(), censor_surv='km')\n",
    "    concordance = ev.concordance_td()\n",
    "    \n",
    "    time_grid = np.linspace(test_durations.min().item(), test_durations.max().item(), 10)\n",
    "    integrated_brier_score = ev.integrated_brier_score(time_grid)\n",
    "    \n",
    "    return concordance, integrated_brier_score, surv_pred\n",
    "\n",
    "print(\"Training functions defined âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation...\n",
      "\n",
      "==================================================\n",
      "FOLD 1/3\n",
      "==================================================\n",
      "Train: 512, Val: 129, Test: 321\n",
      "Test target distribution: [252  69]\n",
      "ALS Clinical Trials Model contains 577,597 trainable parameters\n",
      "Training for 30 epochs with lr 0.001000\n",
      "0:\t[5s / 5s],\t\ttrain_loss: 2.0568\n",
      "1:\t[0s / 6s],\t\ttrain_loss: 1.8059\n",
      "2:\t[0s / 6s],\t\ttrain_loss: 1.7009\n",
      "3:\t[0s / 6s],\t\ttrain_loss: 2.2675\n",
      "4:\t[0s / 6s],\t\ttrain_loss: 1.9945\n",
      "5:\t[0s / 6s],\t\ttrain_loss: 1.7346\n",
      "6:\t[0s / 6s],\t\ttrain_loss: 1.8518\n",
      "7:\t[0s / 6s],\t\ttrain_loss: 1.7443\n",
      "8:\t[0s / 7s],\t\ttrain_loss: 1.7317\n",
      "9:\t[0s / 7s],\t\ttrain_loss: 1.9512\n",
      "Training for 30 epochs with lr 0.000100\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 1.9861\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 1.7099\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 2.2090\n",
      "13:\t[0s / 0s],\t\ttrain_loss: 1.7207\n",
      "14:\t[0s / 0s],\t\ttrain_loss: 1.5372\n",
      "15:\t[0s / 0s],\t\ttrain_loss: 1.4967\n",
      "16:\t[0s / 0s],\t\ttrain_loss: 1.4629\n",
      "17:\t[0s / 1s],\t\ttrain_loss: 1.2888\n",
      "18:\t[0s / 1s],\t\ttrain_loss: 1.3660\n",
      "19:\t[0s / 1s],\t\ttrain_loss: 1.4003\n",
      "Training for 30 epochs with lr 0.000010\n",
      "20:\t[0s / 0s],\t\ttrain_loss: 2.3173\n",
      "21:\t[0s / 0s],\t\ttrain_loss: 1.9155\n",
      "22:\t[0s / 0s],\t\ttrain_loss: 1.9816\n",
      "23:\t[0s / 0s],\t\ttrain_loss: 2.0603\n",
      "24:\t[0s / 0s],\t\ttrain_loss: 2.1572\n",
      "25:\t[0s / 0s],\t\ttrain_loss: 1.5535\n",
      "26:\t[0s / 0s],\t\ttrain_loss: 1.5887\n",
      "27:\t[0s / 1s],\t\ttrain_loss: 2.0828\n",
      "28:\t[0s / 1s],\t\ttrain_loss: 1.6726\n",
      "29:\t[0s / 1s],\t\ttrain_loss: 1.6162\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Run cross-validation\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting cross-validation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m cv_results, trained_models = \u001b[43mrun_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Display overall results\u001b[39;00m\n\u001b[32m     64\u001b[39m results_df = pd.DataFrame(cv_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mrun_cross_validation\u001b[39m\u001b[34m(X, y, phase_groups, config)\u001b[39m\n\u001b[32m     32\u001b[39m coxph_model, logs = train_model(X_train, y_train, X_val, y_val, model, config)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m concordance, brier_score, surv_pred = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoxph_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConcordance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconcordance\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(coxph_model, X_test, y_test)\u001b[39m\n\u001b[32m     87\u001b[39m test_events = tc.tensor(y_test, dtype=tc.float32)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Compute baseline hazards and predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m _ = \u001b[43mcoxph_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_baseline_hazards\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m surv_pred = coxph_model.predict_surv_df(X_test_tensor)\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Compute evaluation metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ct_als/lib/python3.11/site-packages/pycox/models/cox.py:86\u001b[39m, in \u001b[36m_CoxBase.compute_baseline_hazards\u001b[39m\u001b[34m(self, input, target, max_duration, sample, batch_size, set_hazards, eval_, num_workers)\u001b[39m\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNeed to give a \u001b[39m\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to this function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m     \u001b[38;5;28minput\u001b[39m, target = \u001b[38;5;28mself\u001b[39m.training_data\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.sort_values(self.duration_col)\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample >= \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ct_als/lib/python3.11/site-packages/pycox/models/cox.py:59\u001b[39m, in \u001b[36m_CoxBase.target_to_df\u001b[39m\u001b[34m(self, target)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtarget_to_df\u001b[39m(\u001b[38;5;28mself\u001b[39m, target):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     durations, events = \u001b[43mtt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtuplefy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     df = pd.DataFrame({\u001b[38;5;28mself\u001b[39m.duration_col: durations, \u001b[38;5;28mself\u001b[39m.event_col: events}) \n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ct_als/lib/python3.11/site-packages/torchtuples/tupletree.py:430\u001b[39m, in \u001b[36mTupleTree.to_numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type() \u001b[38;5;129;01mis\u001b[39;00m np.ndarray:\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ct_als/lib/python3.11/site-packages/torchtuples/tupletree.py:33\u001b[39m, in \u001b[36mapply_leaf.<locals>.wrapper\u001b[39m\u001b[34m(data, *args, **kwargs)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(data, *args, **kwargs):\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01min\u001b[39;00m _CONTAINERS:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTupleTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ct_als/lib/python3.11/site-packages/torchtuples/tupletree.py:33\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(data, *args, **kwargs):\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01min\u001b[39;00m _CONTAINERS:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m TupleTree(\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ct_als/lib/python3.11/site-packages/torchtuples/tupletree.py:34\u001b[39m, in \u001b[36mapply_leaf.<locals>.wrapper\u001b[39m\u001b[34m(data, *args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01min\u001b[39;00m _CONTAINERS:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TupleTree(wrapper(sub, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ct_als/lib/python3.11/site-packages/torchtuples/tupletree.py:156\u001b[39m, in \u001b[36mtensor_to_numpy\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01mis\u001b[39;00m torch.Size:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(data)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def run_cross_validation(X, y, phase_groups, config):\n",
    "    \"\"\"Run stratified k-fold cross-validation\"\"\"\n",
    "    \n",
    "    # Create stratified folds\n",
    "    skf = StratifiedKFold(n_splits=config['splits'], shuffle=True, random_state=42)\n",
    "    \n",
    "    results = []\n",
    "    models = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, phase_groups)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FOLD {fold + 1}/{config['splits']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold, X_test_fold = X[train_idx], X[test_idx]\n",
    "        y_train_fold, y_test_fold = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Further split training data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_fold, y_train_fold, test_size=0.2, random_state=42, \n",
    "            stratify=phase_groups[train_idx]\n",
    "        )\n",
    "        \n",
    "        print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test_fold)}\")\n",
    "        print(f\"Test target distribution: {np.bincount(y_test_fold.astype(int))}\")\n",
    "        \n",
    "        # Create and train model\n",
    "        model = ALS_Model(X.shape[1], config).to(config['device'])\n",
    "        coxph_model, logs = train_model(X_train, y_train, X_val, y_val, model, config)\n",
    "        \n",
    "        # Evaluate model\n",
    "        concordance, brier_score, surv_pred = evaluate_model(coxph_model, X_test_fold, y_test_fold)\n",
    "        \n",
    "        print(f\"\\nFold {fold + 1} Results:\")\n",
    "        print(f\"Concordance: {concordance:.4f}\")\n",
    "        print(f\"Integrated Brier Score: {brier_score:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'fold': fold + 1,\n",
    "            'concordance': concordance,\n",
    "            'brier_score': brier_score,\n",
    "            'n_test': len(X_test_fold),\n",
    "            'n_positive': int(y_test_fold.sum())\n",
    "        })\n",
    "        \n",
    "        models.append({\n",
    "            'fold': fold + 1,\n",
    "            'model': model,\n",
    "            'coxph_model': coxph_model,\n",
    "            'test_idx': test_idx,\n",
    "            'logs': logs\n",
    "        })\n",
    "    \n",
    "    return results, models\n",
    "\n",
    "# Run cross-validation\n",
    "print(\"Starting cross-validation...\")\n",
    "cv_results, trained_models = run_cross_validation(X, y, phase_groups, config)\n",
    "\n",
    "# Display overall results\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(results_df)\n",
    "print(f\"\\nMean Concordance: {results_df['concordance'].mean():.4f} Â± {results_df['concordance'].std():.4f}\")\n",
    "print(f\"Mean Brier Score: {results_df['brier_score'].mean():.4f} Â± {results_df['brier_score'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lrp_explanations(model, X_test, test_idx, feature_names):\n",
    "    \"\"\"Compute LRP explanations for test samples\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    X_test_tensor = tc.tensor(X_test).float().to(config['device'])\n",
    "    \n",
    "    # Forward pass to get predictions\n",
    "    with tc.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "    \n",
    "    # Compute LRP explanations\n",
    "    model.eval()  # Ensure model is in eval mode for LRP\n",
    "    explanations = []\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        # Set iteration for LRP\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, 'iteration'):\n",
    "                module.iteration = i\n",
    "        \n",
    "        # Single sample prediction and explanation\n",
    "        sample = X_test_tensor[i:i+1]\n",
    "        pred = model(sample)\n",
    "        \n",
    "        # Compute LRP\n",
    "        lrp_scores = model.relprop(pred)\n",
    "        explanations.append(lrp_scores.cpu().numpy().flatten())\n",
    "    \n",
    "    explanations = np.array(explanations)\n",
    "    \n",
    "    # Create explanation DataFrame\n",
    "    explanation_df = pd.DataFrame(explanations, columns=feature_names)\n",
    "    explanation_df['prediction'] = predictions.cpu().numpy().flatten()\n",
    "    explanation_df['sample_idx'] = test_idx\n",
    "    \n",
    "    return explanation_df\n",
    "\n",
    "def analyze_feature_importance(explanation_df, feature_names, top_k=10):\n",
    "    \"\"\"Analyze feature importance from LRP explanations\"\"\"\n",
    "    \n",
    "    # Calculate mean absolute importance for each feature\n",
    "    feature_importance = explanation_df[feature_names].abs().mean().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"Top {top_k} Most Important Features:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (feature, importance) in enumerate(feature_importance.head(top_k).items()):\n",
    "        print(f\"{i+1:2d}. {feature:<30} {importance:.6f}\")\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "print(\"LRP analysis functions defined âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LRP explanations for the best performing fold\n",
    "best_fold = cv_results[np.argmax([r['concordance'] for r in cv_results])]\n",
    "best_model_info = trained_models[best_fold['fold'] - 1]\n",
    "\n",
    "print(f\"Analyzing LRP explanations for best fold (Fold {best_fold['fold']})...\")\n",
    "print(f\"Best concordance: {best_fold['concordance']:.4f}\")\n",
    "\n",
    "# Get test data for best fold\n",
    "test_idx = best_model_info['test_idx']\n",
    "X_test_best = X[test_idx]\n",
    "y_test_best = y[test_idx]\n",
    "model_best = best_model_info['model']\n",
    "\n",
    "# Compute LRP explanations\n",
    "explanation_df = compute_lrp_explanations(model_best, X_test_best, test_idx, feature_names)\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(explanation_df, feature_names, top_k=15)\n",
    "\n",
    "print(f\"\\nGenerated explanations for {len(explanation_df)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(cv_results, feature_importance, explanation_df):\n",
    "    \"\"\"Create visualizations of results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Cross-validation results\n",
    "    ax1 = axes[0, 0]\n",
    "    results_df = pd.DataFrame(cv_results)\n",
    "    ax1.bar(range(1, len(results_df) + 1), results_df['concordance'])\n",
    "    ax1.axhline(y=results_df['concordance'].mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {results_df[\"concordance\"].mean():.3f}')\n",
    "    ax1.set_xlabel('Fold')\n",
    "    ax1.set_ylabel('Concordance Index')\n",
    "    ax1.set_title('Cross-Validation Performance')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Feature importance\n",
    "    ax2 = axes[0, 1]\n",
    "    top_features = feature_importance.head(10)\n",
    "    ax2.barh(range(len(top_features)), top_features.values)\n",
    "    ax2.set_yticks(range(len(top_features)))\n",
    "    ax2.set_yticklabels([f.replace('_', ' ')[:20] + '...' if len(f) > 20 else f.replace('_', ' ') \n",
    "                        for f in top_features.index])\n",
    "    ax2.set_xlabel('Mean Absolute LRP Score')\n",
    "    ax2.set_title('Top 10 Most Important Features')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Prediction distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.hist(explanation_df['prediction'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(x=explanation_df['prediction'].mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {explanation_df[\"prediction\"].mean():.3f}')\n",
    "    ax3.set_xlabel('Model Prediction')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Distribution of Model Predictions')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. LRP score distribution for top feature\n",
    "    ax4 = axes[1, 1]\n",
    "    top_feature = feature_importance.index[0]\n",
    "    lrp_scores = explanation_df[top_feature]\n",
    "    ax4.hist(lrp_scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax4.axvline(x=lrp_scores.mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {lrp_scores.mean():.4f}')\n",
    "    ax4.set_xlabel('LRP Score')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title(f'LRP Score Distribution: {top_feature.replace(\"_\", \" \")}')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "plot_results(cv_results, feature_importance, explanation_df)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset: {X.shape[0]} ALS clinical trials with {X.shape[1]} features\")\n",
    "print(f\"Target: Phase 3+ progression ({y.sum():.0f}/{len(y)} = {y.mean()*100:.1f}% success rate)\")\n",
    "print(f\"Model Performance: {results_df['concordance'].mean():.4f} Â± {results_df['concordance'].std():.4f} concordance\")\n",
    "print(f\"Top predictive feature: {feature_importance.index[0]}\")\n",
    "print(f\"Number of important features (>0.001): {(feature_importance > 0.001).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_features(explanation_df, feature_names, df, top_k=5):\n",
    "    \"\"\"Detailed analysis of top contributing features\"\"\"\n",
    "    \n",
    "    feature_importance = explanation_df[feature_names].abs().mean().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"DETAILED FEATURE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, feature in enumerate(feature_importance.head(top_k).index):\n",
    "        print(f\"\\n{i+1}. {feature}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # LRP statistics\n",
    "        lrp_scores = explanation_df[feature]\n",
    "        print(f\"LRP Score Statistics:\")\n",
    "        print(f\"  Mean: {lrp_scores.mean():.6f}\")\n",
    "        print(f\"  Std:  {lrp_scores.std():.6f}\")\n",
    "        print(f\"  Range: [{lrp_scores.min():.6f}, {lrp_scores.max():.6f}]\")\n",
    "        \n",
    "        # Feature interpretation\n",
    "        if feature.endswith('_1.0') or feature.endswith('_True'):\n",
    "            base_feature = feature.replace('_1.0', '').replace('_True', '')\n",
    "            print(f\"  Type: Binary indicator for '{base_feature}'\")\n",
    "        elif any(phase in feature for phase in ['_I', '_Ii', '_Iii', '_Iv']):\n",
    "            print(f\"  Type: Trial phase indicator\")\n",
    "        elif 'Status_' in feature:\n",
    "            print(f\"  Type: Trial status indicator\")\n",
    "        else:\n",
    "            print(f\"  Type: Feature encoding\")\n",
    "        \n",
    "        # Correlation with target\n",
    "        if len(lrp_scores) > 1:\n",
    "            # Get corresponding actual outcomes for test samples\n",
    "            test_outcomes = y[explanation_df['sample_idx']]\n",
    "            correlation = np.corrcoef(lrp_scores, test_outcomes)[0, 1]\n",
    "            print(f\"  Correlation with outcome: {correlation:.4f}\")\n",
    "\n",
    "# Run detailed analysis\n",
    "analyze_top_features(explanation_df, feature_names, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for future analysis\n",
    "def save_results():\n",
    "    \"\"\"Save analysis results\"\"\"\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs('als_results', exist_ok=True)\n",
    "    \n",
    "    # Save cross-validation results\n",
    "    pd.DataFrame(cv_results).to_csv('als_results/cv_results.csv', index=False)\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv('als_results/feature_importance.csv')\n",
    "    \n",
    "    # Save LRP explanations\n",
    "    explanation_df.to_csv('als_results/lrp_explanations.csv', index=False)\n",
    "    \n",
    "    # Save model configuration\n",
    "    with open('als_results/config.txt', 'w') as f:\n",
    "        for key, value in config.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(\"Results saved to 'als_results/' directory\")\n",
    "\n",
    "# Uncomment to save results\n",
    "# save_results()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Analysis Complete!\")\n",
    "print(\"You can now explore the LRP explanations to understand which\")\n",
    "print(\"trial characteristics contribute most to Phase 3+ progression.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct_als",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
